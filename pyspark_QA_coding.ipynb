{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312d8164-4bf9-4003-baf0-9d4a077d3fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('fs.azure.account.key.storemysynapsedata.dfs.core.windows.net',\"YFmWW1rUyjQ3oVlKUT9nt3IIL/YChFOHOYFhAzsxj4Qwx0+34lQsTP6aJFpEM5YnERglw+7nrsmF+AStya1S6g==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857358be-c245-44f1-88fb-e11adaffc1e8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764179391856}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, sum, array, collect_list\n",
    "\n",
    "df = spark.read.json(\"abfss://raw@storemysynapsedata.dfs.core.windows.net/orders.json\", multiLine=True)\n",
    "df = df.withColumn('items_new', explode('items'))\n",
    "df2 = df.select(\n",
    "    col(\"customer.email\").alias(\"email\"),\n",
    "    col(\"customer.name\").alias(\"customer_name\"),\n",
    "    col(\"delivery.address\").alias(\"address\"),\n",
    "    col(\"delivery.status\").alias(\"delivery_status\"),\n",
    "    col(\"items_new.item\").alias(\"item\"),\n",
    "    col(\"items_new.price\").alias(\"price\"),\n",
    "    col(\"items_new.qty\").alias(\"qty\"),\n",
    "    col(\"order_id\").alias(\"order_id\")\n",
    ")\n",
    "# df2.display()\n",
    "\n",
    "df_total_revenue = df2.groupBy('email').agg(sum(col('price') * col('qty')).alias('total_revenue'))\n",
    "df_total_quantity = df2.groupBy('item').agg(sum(col('qty')).alias('total_quantity'))\n",
    "df_total_revenue_per_item=df2.groupBy('item').agg(sum(col('price')*col('qty')).alias('total_revenue_per_item'))\n",
    "# df5=df2.withColumn('items_purchased', array('item')).withColumn('total_revenue', col('price')*col('qty')) --nooo\n",
    "df_customer_summary=df2.groupBy('email','customer_name').agg(sum(col('price')*col('qty')).alias(\"total_revenue\"),collect_list('item').alias('items_purchased'))\n",
    "\n",
    "\n",
    "df_customer_summary.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9f6c0d-3ff4-465f-92f8-7f992c9b5b3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COUNT TIGER"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Tiger tger Tigre\"),\n",
    "    (2, \"Tigers Tigress Tiger Tiger Tiger\"),\n",
    "    (3, \"Tiger tger Tigre Tigerees\"),\n",
    "    (4, \"Tigers Tiger tiger\"),\n",
    "    (5, \"Tigre Tiger Tigress tiger\")\n",
    "]\n",
    "\n",
    "schema=['id','text']\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a06d02-fae7-48c2-ae47-a4bbf218c959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lower\n",
    "\n",
    "df1=df.withColumn('Tiger_array', split(df.text,\" \"))\n",
    "df2 = df1.withColumn('text_into_rows', explode('Tiger_array'))\n",
    "df3=df2.filter(lower(df2.text_into_rows) == 'tiger')\n",
    "\n",
    "df4=df3.groupBy('id','text').count()\n",
    "df4.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d0c651-3073-47c0-a29d-0822f05fa7b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "salary is higher than their department’s average salary."
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 50000),\n",
    "    (2, \"Bob\", \"HR\", 60000),\n",
    "    (3, \"Charlie\", \"IT\", 80000),\n",
    "    (4, \"David\", \"IT\", 75000),\n",
    "    (5, \"Eve\", \"Finance\", 90000),\n",
    "    (6, \"Frank\", \"Finance\", 85000),\n",
    "    (7, \"Grace\", \"IT\", 95000),\n",
    "]\n",
    "columns = [\"emp_id\", \"name\", \"department\", \"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data, columns)\n",
    "# df=df.groupBy('department').agg(avg('salary').alias('avg_salary'))\n",
    "# df2\n",
    "window_spec=Window.partitionBy('department')\n",
    "df2=df.withColumn('avg_salary',avg('salary').over(window_spec))\n",
    "df2.filter(df2.salary>df2.avg_salary).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d733e2d6-79fd-440a-a4e2-76a22828c391",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "✅ Remove Duplicates From Dataset (using Window Functions)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Playground\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 50000, \"2024-01-01\"),\n",
    "    (1, \"Alice\", \"HR\", 52000, \"2024-03-01\"),\n",
    "    (2, \"Bob\", \"IT\", 70000, \"2024-02-10\"),\n",
    "    (2, \"Bob\", \"IT\", 70000, \"2024-02-10\"),\n",
    "    (3, \"Charlie\", \"Finance\", 90000, \"2024-05-05\"),\n",
    "    (3, \"Charlie\", \"Finance\", 88000, \"2024-01-10\"),\n",
    "]\n",
    "\n",
    "columns = [\"emp_id\", \"name\", \"department\", \"salary\", \"updated_at\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14fc1e6-b7c5-416d-8bcf-41835b50d3d9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764220784482}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn('updated_at', F.col('updated_at').cast('date'))\n",
    "# df=df.withColumn('updated_at', F.to_date(F.col('updated_at')))\n",
    "\n",
    "Window1=Window.partitionBy('emp_id').orderBy(F.col('updated_at').desc())\n",
    "\n",
    "df1=df.withColumn('rank',F.row_number().over(Window1))\n",
    "df2=df1.filter(df1.rank==1).drop('rank')\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06c07f7-9075-43ab-8a5b-332fe9e95a4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "word count logic:"
    }
   },
   "outputs": [],
   "source": [
    "lines = [\n",
    "    \"Hello world!, my world is coding world\",\n",
    "    \"Hello PySpark, excuse me Hello\",\n",
    "    \"PySpark is fun. and fun is writing pyspark code\",\n",
    "    \"Hello!, Hello world again\"\n",
    "]\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WordCountDF\").getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(lines, \"string\").toDF(\"line\")\n",
    "\n",
    "# Your code starts here\n",
    "# 1. Convert to lowercase, split lines into words, explode, filter empty strings\n",
    "# 2. Group by word and count\n",
    "# 3. Sort by count descending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af507a1-377d-4218-93b4-281f676f5210",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764221391349}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lower, col\n",
    "\n",
    "df1=df.withColumn('new', split(lower(df.line),' '))\n",
    "df2=df1.withColumn('new2',explode(df1.new))\n",
    "df3=df2.groupBy('new2').count()\n",
    "df3=df3.sort(col('count').desc()) # sort, orderBy does same work. note: To do desc() we need to specify inside sort() or orderBy()\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cba56f9-9751-4bfc-afcb-2ae943a4d937",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group By and Aggregate List"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GroupByAggregateList\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", \"Recruitment\"),\n",
    "    (2, \"Bob\", \"HR\", \"Training\"),\n",
    "    (3, \"Charlie\", \"IT\", \"Migration\"),\n",
    "    (4, \"David\", \"IT\", \"Security\"),\n",
    "    (5, \"Frank\", \"Finance\", \"Budgeting\"),\n",
    "    (5, \"Frank\", \"Finance\", \"Reporting\"),\n",
    "    (7, \"Grace\", \"IT\", \"Reporting\"),\n",
    "    (7, \"Grace\", \"IT\", \"Automation\")\n",
    "]\n",
    "\n",
    "columns = [\"emp_id\", \"name\", \"department\", \"project\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your task: group by department and aggregate projects into a list\n",
    "# df_result = ?\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29aac02-5e6d-4f12-bcc0-ad1ac65dbc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.groupBy('emp_id','name','department').agg(F.array_sort(collect_list('project')).alias('project_list'))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3341cc-b166-4005-afd9-c660407cf3f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "“Group Records and Aggregate Arrays with Sorting in PySpark”"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AdvancedGroupByArrays\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"Laptop\", 1200),\n",
    "    (1, \"Alice\", \"Mouse\", 25),\n",
    "    (1, \"Alice\", \"Keyboard\", 75),\n",
    "    (2, \"Bob\", \"Monitor\", 300),\n",
    "    (2, \"Bob\", \"Mouse\", 25),\n",
    "    (3, \"Charlie\", \"Laptop\", 1300),\n",
    "    (3, \"Charlie\", \"Mouse\", 30),\n",
    "    (3, \"Charlie\", \"Desk\", 200)\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"customer_name\", \"product\", \"amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your task:\n",
    "# 1. Group by customer_id (or customer_name)\n",
    "# 2. Aggregate product names into a list\n",
    "# 3. Aggregate amounts into a list\n",
    "# 4. Compute total_spent\n",
    "# 5. Sort product list alphabetically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e4ee00-fe61-4a83-8868-3a832112659e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764223063078}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>customer_name</th><th>products</th><th>amounts</th><th>total_spent</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>List(Laptop, Mouse, Keyboard)</td><td>List(1200, 25, 75)</td><td>1300</td></tr><tr><td>2</td><td>Bob</td><td>List(Monitor, Mouse)</td><td>List(300, 25)</td><td>325</td></tr><tr><td>3</td><td>Charlie</td><td>List(Laptop, Mouse, Desk)</td><td>List(1300, 30, 200)</td><td>1530</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         [
          "Laptop",
          "Mouse",
          "Keyboard"
         ],
         [
          1200,
          25,
          75
         ],
         1300
        ],
        [
         2,
         "Bob",
         [
          "Monitor",
          "Mouse"
         ],
         [
          300,
          25
         ],
         325
        ],
        [
         3,
         "Charlie",
         [
          "Laptop",
          "Mouse",
          "Desk"
         ],
         [
          1300,
          30,
          200
         ],
         1530
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "products",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "amounts",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "total_spent",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NO SORTING BECAUSE SORTING MAY REARRANGE THE PRODUCTS->AMOUNTS CORRESPONDENCE\n",
    "# (because sorting the product list separately could break the correspondence between products and amounts)\n",
    "\n",
    "df1=df.groupBy('customer_id','customer_name').agg(F.collect_list('product').alias('products'), F.collect_list('amount').alias('amounts'), F.sum('amount').alias('total_spent'))\n",
    "\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7945f807-638a-4c02-8ae3-4235e610b340",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764223413594}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>customer_name</th><th>product</th><th>amount</th><th>total_spent</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>List(Keyboard, Laptop, Mouse)</td><td>List(75, 1200, 25)</td><td>1300</td></tr><tr><td>2</td><td>Bob</td><td>List(Monitor, Mouse)</td><td>List(300, 25)</td><td>325</td></tr><tr><td>3</td><td>Charlie</td><td>List(Desk, Laptop, Mouse)</td><td>List(200, 1300, 30)</td><td>1530</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         [
          "Keyboard",
          "Laptop",
          "Mouse"
         ],
         [
          75,
          1200,
          25
         ],
         1300
        ],
        [
         2,
         "Bob",
         [
          "Monitor",
          "Mouse"
         ],
         [
          300,
          25
         ],
         325
        ],
        [
         3,
         "Charlie",
         [
          "Desk",
          "Laptop",
          "Mouse"
         ],
         [
          200,
          1300,
          30
         ],
         1530
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "total_spent",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SAFE SORTING\n",
    "\n",
    "df1=df.groupBy('customer_id','customer_name').agg(F.array_sort(F.collect_list(F.struct('product','amount'))).alias('product_amounts'), F.sum('amount').alias('total_spent'))\n",
    "df2=df1.select('customer_id','customer_name',\"product_amounts.product\",\"product_amounts.amount\",'total_spent')\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22e7584-e4f5-46cb-a475-64ad2abd3a31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "\uD83D\uDD25 Monthly Transaction Summary – PySpark (Premium)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MonthlyTransactionSummary\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"2025-01-15\", 200),\n",
    "    (2, \"Bob\", \"2025-01-20\", 300),\n",
    "    (3, \"Charlie\", \"2025-02-05\", 150),\n",
    "    (4, \"David\", \"2025-02-18\", 400),\n",
    "    (5, \"Eve\", \"2025-03-01\", 500),\n",
    "    (6, \"Frank\", \"2025-03-15\", 250),\n",
    "    (7, \"Grace\", \"2025-03-20\", 100)\n",
    "]\n",
    "\n",
    "columns = [\"transaction_id\", \"customer_name\", \"transaction_date\", \"amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Ensure transaction_date is a DateType\n",
    "# df = df.withColumn(\"transaction_date\", F.to_date(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Your task:\n",
    "# 1. Extract month and year\n",
    "# 2. Group by year and month\n",
    "# 3. Compute total transactions, total amount, and average amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ebb7c9-36d0-45a7-b651-800e0a633093",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764224176156}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>month</th><th>total_transactions</th><th>total_amount</th><th>average_amount</th></tr></thead><tbody><tr><td>2025</td><td>1</td><td>2</td><td>500</td><td>250.0</td></tr><tr><td>2025</td><td>2</td><td>2</td><td>550</td><td>275.0</td></tr><tr><td>2025</td><td>3</td><td>3</td><td>850</td><td>283.3333333333333</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2025,
         1,
         2,
         500,
         250.0
        ],
        [
         2025,
         2,
         2,
         550,
         275.0
        ],
        [
         2025,
         3,
         3,
         850,
         283.3333333333333
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_transactions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "average_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure transaction_date is a DateType\n",
    "df = df.withColumn(\"transaction_date\", F.to_date(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "df1=df.withColumn('YEAR', F.year(df.transaction_date)).withColumn('MONTH', F.month(df.transaction_date)) #.drop(df.transaction_date)\n",
    "df2=df1.groupBy('year','month').agg(F.count('*').alias('total_transactions'), F.sum('amount').alias('total_amount'), F.avg('amount').alias('average_amount'))\n",
    "\n",
    "# df2=df1.groupBy('year','month').agg({\"*\":\"count\", \"amount\": \"sum\", \"amount\":\"avg\"}) # KEYS ARE REPEATING IT CONSIDERS LATEST KEY WHICH MISSES total_sales (\"amount\": \"sum\")\n",
    "\n",
    "\n",
    "df2.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_QA_coding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}